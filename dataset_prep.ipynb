{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01-Aug-20', '01-Feb-21.txt', '01-Jun-21.txt', '01-Mar-21.txt', '02-Aug-21.txt', '03-May-21.txt', '04-Jan-21.txt', '05-Apr-21.txt', '06-Sep-21.txt', '07-Dec-20.txt', '07-Jun-21.txt', '08-Feb-21.txt', '08-Mar-21.txt', '10-Aug-21.txt', '10-Jan-22.txt', '10-May-21.txt', '11-Jan-21.txt', '12-Apr-21.txt', '12-Jul-21.txt', '12-Oct-21.txt', '13-Dec-21.txt', '13-Jun-21.txt', '13-Sep-21.txt', '14-Dec-20.txt', '15-Feb-21.txt', '15-Mar-21.txt', '16-Nov-21.txt', '17-Aug-21.txt', '17-May-21.txt', '17-Oct-20.txt', '18-Jan-21.txt', '19-Apr-21.txt', '19-Jul-21.txt', '19-Oct-21.txt', '20-Dec-21.txt', '20-Jun-21.txt', '20-Sep-21.txt', '21-Dec-20.txt', '22-Feb-21.txt', '22-Mar-21.txt', '23 Aug 2020.csv', '23-Aug-20', '23-Nov-20.txt', '23-Nov-21.txt', '24-Aug-21 - P45E.xlsx', '24-Aug-21.txt', '24-Oct-20.txt', '25-Jan-21.txt', '25-May-21.txt', '26-Jul-21.txt', '26-Oct-21.txt', '27-Dec-21.txt', '27-Jun-21.txt', '28-Dec-20.txt', '28-Sep-21.txt', '29-Mar-21.txt', '3-Jan-22.txt', '30-Aug-21.txt', '30-Nov-20.txt', '30-Nov-21.txt', '5-Oct-21.txt', '6-Dec-21.txt', 'VS1DCS_220110145344.CSV']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, datetime, importlib \n",
    "calendar = pd.read_excel('')\n",
    "source = \"\"\n",
    "print(os.listdir(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(filename):\n",
    "    print(f'Retrieving {filename}...')\n",
    "    data = pd.read_csv(source+filename,sep=',',encoding='unicode_escape')\n",
    "    data['Date/Time'] = pd.to_datetime(data['Date/Time'],format='%d/%m/%Y %H:%M:%S')\n",
    "    data['Date/Time'] += datetime.timedelta(hours=8) # to GMT+8\n",
    "    data['Date'] = pd.to_datetime(data['Date/Time']).dt.date\n",
    "    data['Time'] = pd.to_datetime(data['Date/Time']).dt.time\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Shift'] = \"\"\n",
    "    data['Failure Mode'] = data['Desc2']\n",
    "    data['Failure Mode']= data['Failure Mode'].str.replace('\\d+', '')\n",
    "    data['TimeofAlarm']=data[\"Date/Time\"].dt.hour\n",
    "    data=data.loc[(data['Attribute'] == 'ALARMS.MACK') | (data[\"Event Type\"]==\"ALARM\") & (data[\"State\"]==\"ACT/ACK\") | (data[\"State\"]==\"ACT/UNACK\")]\n",
    "    data=data.drop(['FracSec', 'Ord', 'IsArchived','Event_SubType'],axis=1)\n",
    "    data['Week_category']='This week'\n",
    "    # Shift\n",
    "    data.loc[(data['TimeofAlarm'] < 7) | (data['TimeofAlarm'] == 23), 'Shift'] ='N'\n",
    "    data.loc[(data['TimeofAlarm'] >= 7) & (data['TimeofAlarm'] < 15) , 'Shift'] ='M'\n",
    "    data.loc[(data['TimeofAlarm'] >= 15) & (data['TimeofAlarm'] < 23) , 'Shift'] ='A'\n",
    "    print(f'Retrieved!')\n",
    "    ### filter by alarm state too\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date='10-Jan-22.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 10-Jan-22.txt...\n",
      "Retrieved!\n"
     ]
    }
   ],
   "source": [
    "df_alm=get_csv(latest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Team Number and Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teams(df):\n",
    "    \n",
    "    df=df_alm\n",
    "    \n",
    "    df_MA=df.loc[df['Shift'] != 'N']\n",
    "    \n",
    "    df_MA=pd.merge(df_MA,calendar, on=['Date', 'Shift'], how='inner')\n",
    "    \n",
    "    #fixing night shift date to get correct Shift Team\n",
    "\n",
    "# Time of alarm at 23\n",
    "\n",
    "    df_night23=df.loc[(df['Shift'] == 'N') & (df['TimeofAlarm'] == 23)]\n",
    "\n",
    "    df_night23=pd.merge(df_night23,calendar, on=['Date', 'Shift'], how='inner')\n",
    "\n",
    "# time of alarm before 7 (need to minus 1 day)\n",
    "\n",
    "    df_night7=df.loc[(df['Shift'] == 'N') & (df['TimeofAlarm'] < 7)]\n",
    "\n",
    "    df_night7['Date']= df_night7['Date'] - datetime.timedelta(1)\n",
    "\n",
    "    df_night7=pd.merge(df_night7,calendar, on=['Date', 'Shift'], how='inner')\n",
    "\n",
    "    df_night7['Date']= df_night7['Date'] + datetime.timedelta(1)\n",
    "\n",
    "    df_night=pd.concat([df_night23, df_night7])\n",
    "    \n",
    "    df=pd.concat([df_night,df_MA])\n",
    "    \n",
    "    df['Date']=df['Date'].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_alm=get_teams(df_alm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date/Time</th>\n",
       "      <th>Event Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Area</th>\n",
       "      <th>Node</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Module</th>\n",
       "      <th>Module Description</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>State</th>\n",
       "      <th>Level</th>\n",
       "      <th>Desc1</th>\n",
       "      <th>Desc2</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Shift</th>\n",
       "      <th>Failure Mode</th>\n",
       "      <th>TimeofAlarm</th>\n",
       "      <th>Week_category</th>\n",
       "      <th>Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-03 23:21:49</td>\n",
       "      <td>ALARM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PB1_TRAIN5</td>\n",
       "      <td>PB1CT020</td>\n",
       "      <td>T8521</td>\n",
       "      <td>ZAL-2289599</td>\n",
       "      <td>T8521 Chargepoint ZSL</td>\n",
       "      <td>DI_ALM</td>\n",
       "      <td>ACT/UNACK</td>\n",
       "      <td>11-WARNING</td>\n",
       "      <td>Digital Alarm</td>\n",
       "      <td>Value OPEN, Normal CLOSED</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>23:21:49</td>\n",
       "      <td>N</td>\n",
       "      <td>Value OPEN, Normal CLOSED</td>\n",
       "      <td>23</td>\n",
       "      <td>This week</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-03 23:23:48</td>\n",
       "      <td>ALARM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PB1_TRAIN5</td>\n",
       "      <td>PB1CT020</td>\n",
       "      <td>T8521</td>\n",
       "      <td>ZAL-2289599</td>\n",
       "      <td>T8521 Chargepoint ZSL</td>\n",
       "      <td>DI_ALM</td>\n",
       "      <td>ACT/UNACK</td>\n",
       "      <td>11-WARNING</td>\n",
       "      <td>Digital Alarm</td>\n",
       "      <td>Value OPEN, Normal CLOSED</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>23:23:48</td>\n",
       "      <td>N</td>\n",
       "      <td>Value OPEN, Normal CLOSED</td>\n",
       "      <td>23</td>\n",
       "      <td>This week</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-03 23:29:39</td>\n",
       "      <td>CHANGE</td>\n",
       "      <td>USER</td>\n",
       "      <td>PB1_TRAIN5</td>\n",
       "      <td>PB1WS013</td>\n",
       "      <td>T8521</td>\n",
       "      <td>ZAL-2289599</td>\n",
       "      <td>T8521 Chargepoint ZSL</td>\n",
       "      <td>ALARMS.MACK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WLK39042</td>\n",
       "      <td>NEW VALUE = 1</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>23:29:39</td>\n",
       "      <td>N</td>\n",
       "      <td>NEW VALUE =</td>\n",
       "      <td>23</td>\n",
       "      <td>This week</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-03 23:29:40</td>\n",
       "      <td>CHANGE</td>\n",
       "      <td>USER</td>\n",
       "      <td>PB1_TRAIN5</td>\n",
       "      <td>PB1WS013</td>\n",
       "      <td>T8521</td>\n",
       "      <td>ZAL-2289599</td>\n",
       "      <td>T8521 Chargepoint ZSL</td>\n",
       "      <td>ALARMS.MACK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WLK39042</td>\n",
       "      <td>NEW VALUE = 1</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>23:29:40</td>\n",
       "      <td>N</td>\n",
       "      <td>NEW VALUE =</td>\n",
       "      <td>23</td>\n",
       "      <td>This week</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-03 23:30:02</td>\n",
       "      <td>ALARM</td>\n",
       "      <td>DEVICE</td>\n",
       "      <td>PB1_LAM3_FAT4</td>\n",
       "      <td>PB1CT006</td>\n",
       "      <td>V79</td>\n",
       "      <td>QX-205596</td>\n",
       "      <td>V-79 Conductivity analyser</td>\n",
       "      <td>ADVISE_ALM</td>\n",
       "      <td>ACT/ACK</td>\n",
       "      <td>07-ADVISORY</td>\n",
       "      <td>ADVISE</td>\n",
       "      <td>Transducer Block Out Of Service</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>23:30:02</td>\n",
       "      <td>N</td>\n",
       "      <td>Transducer Block Out Of Service</td>\n",
       "      <td>23</td>\n",
       "      <td>This week</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date/Time Event Type Category           Area      Node   Unit  \\\n",
       "0 2022-01-03 23:21:49      ALARM      NaN     PB1_TRAIN5  PB1CT020  T8521   \n",
       "1 2022-01-03 23:23:48      ALARM      NaN     PB1_TRAIN5  PB1CT020  T8521   \n",
       "2 2022-01-03 23:29:39     CHANGE     USER     PB1_TRAIN5  PB1WS013  T8521   \n",
       "3 2022-01-03 23:29:40     CHANGE     USER     PB1_TRAIN5  PB1WS013  T8521   \n",
       "4 2022-01-03 23:30:02      ALARM   DEVICE  PB1_LAM3_FAT4  PB1CT006    V79   \n",
       "\n",
       "        Module          Module Description    Attribute      State  \\\n",
       "0  ZAL-2289599       T8521 Chargepoint ZSL       DI_ALM  ACT/UNACK   \n",
       "1  ZAL-2289599       T8521 Chargepoint ZSL       DI_ALM  ACT/UNACK   \n",
       "2  ZAL-2289599       T8521 Chargepoint ZSL  ALARMS.MACK        NaN   \n",
       "3  ZAL-2289599       T8521 Chargepoint ZSL  ALARMS.MACK        NaN   \n",
       "4    QX-205596  V-79 Conductivity analyser   ADVISE_ALM    ACT/ACK   \n",
       "\n",
       "         Level          Desc1                            Desc2        Date  \\\n",
       "0   11-WARNING  Digital Alarm        Value OPEN, Normal CLOSED  2022-01-03   \n",
       "1   11-WARNING  Digital Alarm        Value OPEN, Normal CLOSED  2022-01-03   \n",
       "2          NaN       WLK39042                    NEW VALUE = 1  2022-01-03   \n",
       "3          NaN       WLK39042                    NEW VALUE = 1  2022-01-03   \n",
       "4  07-ADVISORY         ADVISE  Transducer Block Out Of Service  2022-01-03   \n",
       "\n",
       "       Time Shift                     Failure Mode  TimeofAlarm Week_category  \\\n",
       "0  23:21:49     N        Value OPEN, Normal CLOSED           23     This week   \n",
       "1  23:23:48     N        Value OPEN, Normal CLOSED           23     This week   \n",
       "2  23:29:39     N                     NEW VALUE =            23     This week   \n",
       "3  23:29:40     N                     NEW VALUE =            23     This week   \n",
       "4  23:30:02     N  Transducer Block Out Of Service           23     This week   \n",
       "\n",
       "   Team  \n",
       "0     2  \n",
       "1     2  \n",
       "2     2  \n",
       "3     2  \n",
       "4     2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtered Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date_filtered= f\"{latest_date} - Filtered.txt\"\n",
    "source2=\"//Siwdsntv002/SG_PSC_SG1_PROJECT_NON-GMP/H0137_VS1 Alarms Data Mining_DL/PowerBI//CSV 2021/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alm.to_csv(source2+latest_date_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Historical Data and Latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace This week to Last week in Historical Data\n",
    "history['Week_category'] = history['Week_category'].replace(['This week'],'Last week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Duplicates before Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Last Date\n",
    "def concat(x,y):\n",
    "    x_Date=x.iloc[-1:,14:15].values.tolist()\n",
    "    y_Date=y.iloc[-1:,14:15].values.tolist()\n",
    "# Check if last row dates of both df are the same\n",
    "    if x_Date == y_Date:\n",
    "         raise ValueError('Duplicate found, Please do not attempt to Duplicate')\n",
    "    else: \n",
    "         x=pd.concat([x,y], sort=False)\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=concat(history, df_alm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['Week_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.to_csv(\"xxx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Information from Shift Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHIFT REPORTER\n",
    "from docx2python import docx2python\n",
    "import re\n",
    "import datetime\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "#!pip install docx2python-1.27.1-py3-none-any.whl\n",
    "#!pip install fuzzywuzzy\n",
    " \n",
    "def flatten(x): \n",
    "    '''external function to recursively flatten lists'''\n",
    "    result = []\n",
    "    for el in x:\n",
    "        if hasattr(el, \"__iter__\") and not isinstance(el, (str, bytes)):\n",
    "            result.extend(flatten(el))\n",
    "        else:\n",
    "            if 'href' not in el:\n",
    "                result.append(el.replace('\\xa0',''))\n",
    "    return result\n",
    " \n",
    "def truncate_report(report,split_phrase1,split_phrase2):#=None):\n",
    "    '''func used in __init__ method to truncate report before split_phrase'''\n",
    "    try:\n",
    "#         if split_phrase2 is not None:\n",
    "#             print('in 1')\n",
    "#             return report[report.index(split_phrase1)+1:] ## to remove\n",
    "        return report[report.index(split_phrase1)+1:report.index(split_phrase2)]\n",
    "    except:\n",
    "        return report\n",
    "\n",
    "def get_keywords(df_alarms, module):\n",
    "    '''Takes an alarms dataframe and module as string\n",
    "        Returns regex pattern of Module | Unit | Module Description as string\n",
    "        To be used to search shiftreport\n",
    "    '''\n",
    "    # regex can be improved for edge cases with TIA-xxxx_CPM for certain modules\n",
    "    \n",
    "    keywords = [module.replace('-','')]\n",
    "\n",
    "    unit_pattern = re.compile(r\"[a-zA-z]{1,}-?\\d{1,}\")\n",
    "    try:\n",
    "        for col in ['Unit','Module Description']:\n",
    "            results = df_alarms[df_alarms['Module'] == module][col].unique().tolist()\n",
    "\n",
    "            # for module description, use re to extract potential units i.e. M1234, HE101\n",
    "            if col == 'Module Description':\n",
    "                for result in results:\n",
    "                    keywords += re.findall(unit_pattern,result)\n",
    "\n",
    "            keywords += results #concat all to list\n",
    "        keywords = set([kw.replace('-','') for kw in keywords if type(kw)==str ]) # remove duplicates\n",
    "\n",
    "\n",
    "        kw_pattern = fr\"{'|'.join(keywords)}\"\n",
    "    except:\n",
    "        #to return module for cases where description etc are NaN. This will prevent subsequent errors\n",
    "        kw_pattern = keywords\n",
    "        \n",
    "    return kw_pattern\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shift_report:\n",
    "    '''class shift report is initiated by (filename)\n",
    "    methods:\n",
    "        shift_report()\n",
    " \n",
    "    '''\n",
    "    def __init__(self,filename):\n",
    "        \n",
    "        # Regex pattern for date extraction\n",
    "        pattern = re.compile(r\"\\d+-[a-zA-Z]+-\\d+,? ([Mm]orning|[Aa]fternoon|[Nn]ight)\\s*$\")\n",
    "        \n",
    "        doc2x = docx2python(filename)\n",
    "        self.shiftreport = flatten(doc2x.body) # recursively flatten list\n",
    "        \n",
    "        #docx created on\n",
    "        #self.created_on = #datetime.datetime.strptime(\n",
    "            #doc2x['created'],\"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%d-%b-%Y %H:%M\") \n",
    "     \n",
    "        self.dates = list(filter(pattern.match, self.shiftreport))\n",
    "        # for each date indexed by i, create list within list to be zipped to dataframe\n",
    "        self.shiftreport = [self.shiftreport[self.shiftreport.index(self.dates[i])+1:self.shiftreport.index(\n",
    "            self.dates[i+1])] if i < (len(self.dates)-1) else\n",
    "            self.shiftreport[self.shiftreport.index(self.dates[i])+1:] for i in range(len(self.dates))\n",
    "                       ] \n",
    " \n",
    "        srdf = pd.DataFrame(zip(self.dates,self.shiftreport),columns=['Date','Report'])\n",
    "        srdf['Report'] = srdf['Report'].apply(lambda x: list(filter(None,x)))\n",
    " \n",
    "        srdf['Reported By'] = srdf['Report'].apply(lambda x: ' '.join(x[1:4]))\n",
    " \n",
    "        #Truncate report\n",
    "        srdf['Report'] = srdf.Report.apply( # Truncate report here\n",
    "            lambda x: truncate_report(x,\"Plant Engineering Work Request  & Details ( WR Raised no. ) \",\n",
    "                                      \"Abnormalities of trends observed:\")) #\"Process Detail for all batches offloaded for trains\"\n",
    "        \n",
    "        # Remove white spaces and '-'\n",
    "        srdf['Report'] = srdf['Report'].apply(\n",
    "            lambda x: [el.replace('-','') for el in x if el not in [' ','']])\n",
    "                                                                # remove white spaces / ##### TO MAKE THIS REGEX #####\n",
    "        \n",
    "        \n",
    "        self.df = srdf.copy()\n",
    "#         print(f'File loaded.. \\\n",
    "#             Exported on {self.created_on}, dates from {self.dates[0]} to {self.dates[-1]}')\n",
    "        self.melt_dataframe(self.df)\n",
    "        print(f'Dataframe melted')\n",
    "        \n",
    "    def melt_dataframe(self, df):\n",
    "        '''Further processing for self.df to melt into single rows'''\n",
    "        \n",
    "        df = df.merge(df['Report'].apply(pd.Series),\n",
    "                    right_index=True,left_index=True).melt(id_vars=['Date','Reported By','Report'],\n",
    "                                                           value_name=\"Text\",var_name=\"Var\")\n",
    "\n",
    "        df = df.drop(['Report','Var'],axis=1)\n",
    "        \n",
    "        df['Shift'] = df['Date'].str[9:]\n",
    "        df['Date'] = df['Date'].str[:9]\n",
    "        df['Date'] = pd.to_datetime(df['Date'],errors='coerce')\n",
    "        df['Shift_'] = df['Shift'].map({'Morning':'01-M',\n",
    "                          'Afternoon':'02-A','Night':'03-N'})\n",
    "        \n",
    "        df = df.sort_values(['Date','Shift_']).drop('Shift_',axis=1)\n",
    "        df = df.dropna(subset=['Date','Text'])\n",
    "        df = df.drop_duplicates(subset=['Text'],keep='first')\n",
    "        \n",
    "        cols = ['Date','Shift','Reported By','Text']\n",
    "        self.df = df.loc[:,cols]\n",
    "    \n",
    "    def search_dataframe(self,keywords):\n",
    "        '''Takes in keywords as regex, returns self.df filtered by keyword matches\n",
    "        '''\n",
    "        print(f'Searching for ... {keywords}')\n",
    "        mask = self.df['Text'].str.contains(keywords,case=False)\n",
    "        results = self.df[mask].copy()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QIA-205596',\n",
       " 'TIA-204005',\n",
       " 'TIA-360149',\n",
       " 'TIA-2284506',\n",
       " 'CVO-200072',\n",
       " 'M8914_DRUMCHG_EM',\n",
       " 'XA-2284040']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alm_shift = df_alm.loc[(df_alm['Level'].isin(['15-CRITICAL','11-WARNING']))]\n",
    "\n",
    "N = 7\n",
    "top_N_modules = df_alm_shift.Module.value_counts()[:N].index.to_list()\n",
    "top_N_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe melted\n",
      "Searching for ... V79|V79 Conductivity analyser|QIA205596\n",
      "Searching for ... TIA204005|V400 HEAT TRACING TEMP|V400\n",
      "Searching for ... BRINE SUPPLY TEMPERATURE|PB1_UTILS|TIA360149\n",
      "Searching for ... HE8570 BRR TX|R8570|TIA2284506|HE8570\n",
      "Searching for ... CVO200072|V1 Top Jkt Vent CVO|V1\n",
      "Searching for ... M8914 Drum Charge EM|M8914_DRUMCHG_EM|M8914\n",
      "Searching for ... XA2284040|R8460 Agi Seal|R8460\n"
     ]
    }
   ],
   "source": [
    "#To be continued\n",
    "# to develop for multiple shift-report objects for different months (sr)\n",
    "#os.listdir()\n",
    "path = \"\"\n",
    "filename = \"Dec 2021.docx\"\n",
    "#doc2x = docx2python(filename)\n",
    "\n",
    "#2x loop for 2 months (overlapping)\n",
    "\n",
    "sr = shift_report(path+filename)\n",
    "results = []\n",
    "for module in top_N_modules:\n",
    "    result = sr.search_dataframe(get_keywords(df_alm,module)).copy()\n",
    "    result['Module'] = module\n",
    "    results.append(result)\n",
    "\n",
    "results = pd.concat(results)\n",
    "\n",
    "# to send to excel file, remove duplicates for different weeks?\n",
    "results.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe melted\n",
      "Searching for ... V79|V79 Conductivity analyser|QIA205596\n",
      "Searching for ... TIA204005|V400 HEAT TRACING TEMP|V400\n",
      "Searching for ... BRINE SUPPLY TEMPERATURE|PB1_UTILS|TIA360149\n",
      "Searching for ... HE8570 BRR TX|R8570|TIA2284506|HE8570\n",
      "Searching for ... CVO200072|V1 Top Jkt Vent CVO|V1\n",
      "Searching for ... M8914 Drum Charge EM|M8914_DRUMCHG_EM|M8914\n",
      "Searching for ... XA2284040|R8460 Agi Seal|R8460\n"
     ]
    }
   ],
   "source": [
    "# to develop for multiple shift-report objects for different months (sr)\n",
    "#os.listdir()\n",
    "path = \"\n",
    "filename = \"Jan 2022.docx\"\n",
    "#doc2x = docx2python(filename)\n",
    "\n",
    "#2x loop for 2 months (overlapping)\n",
    "\n",
    "sr = shift_report(path+filename)\n",
    "results = []\n",
    "for module in top_N_modules:\n",
    "    result = sr.search_dataframe(get_keywords(df_alm,module)).copy()\n",
    "    result['Module'] = module\n",
    "    results.append(result)\n",
    "\n",
    "results = pd.concat(results)\n",
    "\n",
    "# to send to excel file, remove duplicates for different weeks?\n",
    "results.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat large data\n",
    "\n",
    "files = [file for file in os.listdir(source) if '21.txt' in file]\n",
    "df_alm = pd.concat([(get_csv(file)) for file in files])\n",
    "df_alm.sort_values('Date/Time',inplace=True)\n",
    "df_alm.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
